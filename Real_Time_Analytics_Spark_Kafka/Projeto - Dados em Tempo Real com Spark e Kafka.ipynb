{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d2a627",
   "metadata": {},
   "source": [
    "## Prevendo a temperatura de sensores de uma maquina industrial em tempo real\n",
    "\n",
    "Projeto que tem como objetivo demonstrar a utilização do Spark Streaming e do Apache Kafka em conjunto.\n",
    "Este projeto já considera que o o Apache Kafka está rodando no meu terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb278bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09941f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1355e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b654bd",
   "metadata": {},
   "source": [
    "Precisamos incluir o conector de integração do Spark Streaming com o Apache Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ff27d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8180c161",
   "metadata": {},
   "source": [
    "## Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5e6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 16:40:43 WARN Utils: Your hostname, ingo-Vostro-3583 resolves to a loopback address: 127.0.1.1; using 192.168.1.10 instead (on interface wlo1)\n",
      "23/03/23 16:40:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/ingo/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ingo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ingo/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f4dbad97-a872-4909-babc-2432c85c8868;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.0/spark-sql-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0!spark-sql-kafka-0-10_2.12.jar (1306ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.0/spark-token-provider-kafka-0-10_2.12-3.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0!spark-token-provider-kafka-0-10_2.12.jar (436ms)\n",
      ":: resolution report :: resolve 4491ms :: artifacts dl 1785ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   2   |   2   |   0   ||   12  |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f4dbad97-a872-4909-babc-2432c85c8868\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 10 already retrieved (469kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 16:40:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName('Dados em Tempo Real com Spark e Kafka').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea7d31",
   "metadata": {},
   "source": [
    "## Leitura do Kafka Spark Structed Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63bcc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "    .option('subscribe', 'meu_projeto') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ed568",
   "metadata": {},
   "source": [
    "## Definição do Schema da Fonde de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48115bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (temperatura)\n",
    "esquema_dados_temp = StructType([StructField('leitura',\n",
    "                                              StructType([StructField('temperatura', DoubleType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "593d55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema global dos dados no streaming\n",
    "esquema_dados = StructType([\n",
    "    StructField('id_sensor', StringType(), True),\n",
    "    StructField('id_equipamento', StringType(), True),\n",
    "    StructField('sensor', StringType(), True),\n",
    "    StructField('data_evento', StringType(), True),\n",
    "    StructField('padrao', esquema_dados_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afd538",
   "metadata": {},
   "source": [
    "## Parse da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee686a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr('CAST(value AS STRING)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "797e0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn('jsonData', from_json(col('value'), esquema_dados)).select('jsonData.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e653a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- id_equipamento: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- data_evento: string (nullable = true)\n",
      " |-- padrao: struct (nullable = true)\n",
      " |    |-- leitura: struct (nullable = true)\n",
      " |    |    |-- temperatura: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f54ad",
   "metadata": {},
   "source": [
    "## Preparamos o Dataframe\n",
    "Esse dataframe está no formato que precisamos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d00ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_temp_sensor = df_conversao.select(col('padrao.leitura.temperatura').alias('temperatura'),\n",
    "                                               col('sensor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "419ec588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eefbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não podemos visualizar o dataframe, pois a fonte é de streaming\n",
    "# df_conversao_temp_sensor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b2461",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef97a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupBy('sensor').mean('temperatura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0324873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperatura): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd5c9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col('sensor').alias('sensor'),\n",
    "                                                   col('avg(temperatura)').alias('media_temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faf44784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- media_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3f3c1",
   "metadata": {},
   "source": [
    "Abaixo abrimos o straming para análise de dados em tempo real, imprimindo o resultado do console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6658c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 17:25:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2ebb6837-c5d6-47de-9550-7b4fb7e718b3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/23 17:25:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+\n",
      "|sensor|media_temp|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.17142857142856|\n",
      "|sensor34| 85.97999999999999|\n",
      "|sensor41| 64.72500000000001|\n",
      "|sensor50|             59.15|\n",
      "|sensor38|59.800000000000004|\n",
      "|sensor31|             37.65|\n",
      "| sensor1| 39.27142857142858|\n",
      "|sensor10| 60.87500000000001|\n",
      "|sensor30| 68.83333333333333|\n",
      "|sensor25|42.385714285714286|\n",
      "| sensor4| 73.50000000000001|\n",
      "| sensor5|              70.4|\n",
      "|sensor20|48.800000000000004|\n",
      "|sensor44|              38.1|\n",
      "|sensor19|57.833333333333336|\n",
      "| sensor8| 51.03333333333333|\n",
      "|sensor14| 51.34285714285714|\n",
      "|sensor24|             14.15|\n",
      "|sensor43|             56.05|\n",
      "|sensor47|              56.0|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.38762886597935|\n",
      "|sensor34| 84.63955555555556|\n",
      "|sensor41| 64.29948979591838|\n",
      "|sensor50| 58.32546296296296|\n",
      "|sensor31|37.733689839572186|\n",
      "|sensor38|57.922815533980604|\n",
      "| sensor1| 38.27512690355329|\n",
      "|sensor10|  62.8454081632653|\n",
      "|sensor30|  71.8108695652174|\n",
      "|sensor25|43.041752577319606|\n",
      "| sensor4| 73.27192118226601|\n",
      "| sensor5| 71.77276995305165|\n",
      "|sensor20| 49.94117647058822|\n",
      "|sensor44| 40.09814814814817|\n",
      "|sensor19|              58.5|\n",
      "| sensor8| 51.97599999999999|\n",
      "|sensor14|49.149404761904755|\n",
      "|sensor24| 16.58366336633663|\n",
      "|sensor43| 54.79739583333335|\n",
      "|sensor47| 53.75657894736842|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.43010204081631|\n",
      "|sensor34| 84.61629955947137|\n",
      "|sensor41| 64.26915422885573|\n",
      "|sensor50|            58.285|\n",
      "|sensor31| 37.74338624338624|\n",
      "|sensor38| 57.87924528301888|\n",
      "| sensor1| 38.32388059701492|\n",
      "|sensor10| 62.85721393034825|\n",
      "|sensor30| 71.81058201058201|\n",
      "|sensor25| 43.01776649746195|\n",
      "| sensor4| 73.33047619047619|\n",
      "| sensor5| 71.76930232558139|\n",
      "|sensor20| 49.91650943396225|\n",
      "|sensor44| 40.10230414746546|\n",
      "|sensor19|              58.5|\n",
      "| sensor8|51.980676328502405|\n",
      "|sensor14| 49.13274853801169|\n",
      "|sensor24|16.556796116504852|\n",
      "|sensor43| 54.81709844559587|\n",
      "|sensor47| 53.77359307359308|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.66605263157895|\n",
      "|sensor34|  84.4991208791209|\n",
      "|sensor41| 64.40315533980585|\n",
      "|sensor50| 58.17645569620253|\n",
      "|sensor31|  37.8650872817955|\n",
      "|sensor38| 57.86391752577321|\n",
      "| sensor1| 38.28641025641026|\n",
      "|sensor10|62.786330935251804|\n",
      "|sensor30|  71.7271428571429|\n",
      "|sensor25| 42.92268041237115|\n",
      "| sensor4| 73.27254901960787|\n",
      "| sensor5|  71.7095238095238|\n",
      "|sensor20|49.652655889145485|\n",
      "|sensor44|40.166259168704165|\n",
      "|sensor19|  58.7860349127182|\n",
      "| sensor8| 51.87468030690536|\n",
      "|sensor14| 48.98467336683417|\n",
      "|sensor24|16.644575471698108|\n",
      "|sensor43| 54.61506493506494|\n",
      "|sensor47| 53.56560975609756|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao straming com formado de console\n",
    "query = df_media_temp_sensor.writeStream.outputMode('complete').format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ef45c",
   "metadata": {},
   "source": [
    "Envie novos arquivos para o Kafka a fim de ver a análise tempo real por aqui. Clique no botão Stop no menu superior para interromper a célula a qualquer momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "153711e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.67994791666666|\n",
      "|sensor34| 84.48665207877464|\n",
      "|sensor41| 64.40315533980585|\n",
      "|sensor50| 58.15839598997494|\n",
      "|sensor31| 37.88325123152709|\n",
      "|sensor38| 57.86403061224491|\n",
      "| sensor1|38.291857506361325|\n",
      "|sensor10| 62.77995226730311|\n",
      "|sensor30| 71.70649717514127|\n",
      "|sensor25| 42.91074168797955|\n",
      "| sensor4| 73.27170731707321|\n",
      "| sensor5| 71.71592039800994|\n",
      "|sensor20| 49.64919540229884|\n",
      "|sensor44|40.153640776699035|\n",
      "|sensor19|58.774129353233825|\n",
      "| sensor8|51.866071428571416|\n",
      "|sensor14|          49.00125|\n",
      "|sensor24| 16.63294117647058|\n",
      "|sensor43| 54.60719794344474|\n",
      "|sensor47| 53.56496350364963|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7|  80.6403418803419|\n",
      "|sensor34| 84.53696969696969|\n",
      "|sensor41| 64.49641109298534|\n",
      "|sensor50| 58.24337349397591|\n",
      "|sensor31| 37.86589018302829|\n",
      "|sensor38|57.847254575707176|\n",
      "| sensor1| 38.32307692307692|\n",
      "|sensor10|62.739770867430444|\n",
      "|sensor30| 71.64095063985377|\n",
      "|sensor25| 42.90232945091516|\n",
      "| sensor4|  73.4057096247961|\n",
      "| sensor5| 71.63752012882446|\n",
      "|sensor20| 49.51806853582554|\n",
      "|sensor44| 40.12380952380952|\n",
      "|sensor19| 58.82838063439065|\n",
      "| sensor8| 51.97439862542956|\n",
      "|sensor14| 48.97466666666667|\n",
      "|sensor24|16.671617161716167|\n",
      "|sensor43| 54.53056994818654|\n",
      "|sensor47| 53.55258764607679|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ingo/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ingo/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/ingo/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64302/4070293342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Executamos a query do streaming e evitamos que o processo seja encerrado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bee03720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14accfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f61f3fa5-d11e-4176-9432-224e25134e6b',\n",
       " 'runId': '40e2a1de-3d5a-4b81-8a42-d951601d569e',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-03-23T20:41:58.689Z',\n",
       " 'batchId': 7,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 1, 'triggerExecution': 2},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 50,\n",
       "   'numRowsUpdated': 0,\n",
       "   'allUpdatesTimeMs': 2623,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 26508,\n",
       "   'memoryUsedBytes': 101936,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 2400,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 30568}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[meu_projeto]]',\n",
       "   'startOffset': {'meu_projeto': {'0': 40000}},\n",
       "   'endOffset': {'meu_projeto': {'0': 40000}},\n",
       "   'latestOffset': {'meu_projeto': {'0': 40000}},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@4bddb3e7',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8013067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@1379fee4, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2383/0x000000010116d440@2cbbdf09\n",
      "+- *(4) HashAggregate(keys=[sensor#49], functions=[avg(temperatura#57)])\n",
      "   +- StateStoreSave [sensor#49], state info [ checkpoint = file:/tmp/temporary-2ebb6837-c5d6-47de-9550-7b4fb7e718b3/state, runId = 40e2a1de-3d5a-4b81-8a42-d951601d569e, opId = 0, ver = 6, numPartitions = 200], Complete, 0, 2\n",
      "      +- *(3) HashAggregate(keys=[sensor#49], functions=[merge_avg(temperatura#57)])\n",
      "         +- StateStoreRestore [sensor#49], state info [ checkpoint = file:/tmp/temporary-2ebb6837-c5d6-47de-9550-7b4fb7e718b3/state, runId = 40e2a1de-3d5a-4b81-8a42-d951601d569e, opId = 0, ver = 6, numPartitions = 200], 2\n",
      "            +- *(2) HashAggregate(keys=[sensor#49], functions=[merge_avg(temperatura#57)])\n",
      "               +- Exchange hashpartitioning(sensor#49, 200), ENSURE_REQUIREMENTS, [plan_id=1365]\n",
      "                  +- *(1) HashAggregate(keys=[sensor#49], functions=[partial_avg(temperatura#57)])\n",
      "                     +- Project [from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#29 as string), Some(America/Sao_Paulo)).padrao.leitura.temperatura AS temperatura#57, from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#29 as string), Some(America/Sao_Paulo)).sensor AS sensor#49]\n",
      "                        +- MicroBatchScan[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c740e",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e120f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 17:45:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d8dd10e3-ff80-4d09-ad9c-4c688e9101bd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/03/23 17:45:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de memória (cria tabela temporária)\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName('ingo') \\\n",
    "    .outputMode('complete') \\\n",
    "    .format('memory') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3702e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f20104139d0>,\n",
       " <pyspark.sql.streaming.StreamingQuery at 0x7f20103183d0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streams ativados\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9337e7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:====================>                                  (73 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64810863239575|\n",
      "|sensor34|  84.6014869888476|\n",
      "|sensor41| 64.59472636815923|\n",
      "|sensor50|58.377334732423925|\n",
      "|sensor31|37.855588526211676|\n",
      "|sensor38|57.800000000000026|\n",
      "| sensor1|38.283682008368196|\n",
      "|sensor10| 62.78532663316584|\n",
      "|sensor30|  71.6750738916256|\n",
      "|sensor25| 42.73330049261086|\n",
      "| sensor4| 73.39194312796208|\n",
      "| sensor5| 71.67099080694584|\n",
      "|sensor20|49.467289719626166|\n",
      "|sensor44| 40.15437185929648|\n",
      "|sensor19|  58.6866204162537|\n",
      "| sensor8|51.938270377733595|\n",
      "|sensor14| 48.92823061630219|\n",
      "|sensor24| 16.63876739562624|\n",
      "|sensor43|54.412439261418854|\n",
      "|sensor47|53.309783728115335|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:============================>                         (105 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.66400322841002|\n",
      "|sensor34| 84.56037588097104|\n",
      "|sensor41| 64.55382674516402|\n",
      "|sensor50| 58.40918635170603|\n",
      "|sensor31| 37.83206296603149|\n",
      "|sensor38| 57.80048622366291|\n",
      "| sensor1| 38.33978132884777|\n",
      "|sensor10| 62.76018518518519|\n",
      "|sensor30| 71.66691419141914|\n",
      "|sensor25| 42.76210526315791|\n",
      "| sensor4| 73.33364854215918|\n",
      "| sensor5| 71.70967741935483|\n",
      "|sensor20|49.417809298660366|\n",
      "|sensor44|40.148471986417654|\n",
      "|sensor19|58.634428923582576|\n",
      "| sensor8| 52.00951178451178|\n",
      "|sensor14|48.893708053691284|\n",
      "|sensor24|16.649295774647882|\n",
      "|sensor43| 54.44992012779553|\n",
      "|sensor47| 53.38359966358283|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "from time import sleep\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    spark.sql('select sensor, round(media_temp, 2) as media from ingo where media_temp > 65').show()\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e473dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
